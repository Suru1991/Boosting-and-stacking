{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:  What is Boosting in Machine Learning? Explain how it improves weak\n",
        "learners.\n",
        "\n",
        "Answer:Boosting is an ensemble machine learning technique that combines multiple weak learners (models that perform slightly better than random guessing) to create a strong learner. The core idea is to sequentially build models, where each new model corrects the errors of the previous ones. This iterative process gradually improves the overall predictive accuracy.\n",
        "\n",
        "Here's how Boosting improves weak learners:\n",
        "\n",
        "\n",
        "\n",
        "1.   Sequential Learning: Unlike bagging (e.g., Random Forests) where models are built independently, boosting builds models in a sequential manner. Each subsequent model focuses on the data points that were misclassified or poorly predicted by the previous models.\n",
        "\n",
        "2.  Weighted Data/Instances: In each iteration, boosting algorithms assign higher weights to the misclassified data points and lower weights to correctly classified points. This forces the next weak learner to pay more attention to the 'difficult' examples, effectively learning from the mistakes of its predecessors..   \n",
        "3.   Weighted Combination of Weak Learners: The final strong learner is a weighted sum of all the weak learners. Learners that perform better on the training data are given higher weights, contributing more to the final prediction.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.   Error Minimization: By continuously focusing on reducing the residual errors, boosting iteratively refines the model's performance. It transforms a collection of simple, often biased, weak models into a powerful and accurate predictive model.\n",
        "\n"
      ],
      "metadata": {
        "id": "V5w_IhdTLQbz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between AdaBoost and Gradient Boosting in terms\n",
        "of how models are trained?\n",
        "\n",
        "Answer:Both AdaBoost and Gradient Boosting are popular boosting algorithms, but they differ significantly in how they train successive models and minimize errors:\n",
        "\n",
        "**AdaBoost (Adaptive Boosting):**\n",
        "\n",
        "\n",
        "\n",
        "1.   Focus on Misclassified Samples: AdaBoost primarily focuses on incorrectly classified samples from the previous weak learner. It assigns higher weights to these misclassified samples in the subsequent training iteration.\n",
        "\n",
        "2.   Weighted Voting: Each weak learner is trained on the re-weighted dataset. After training, a weight is assigned to the weak learner itself based on its accuracy. More accurate learners get higher weights.\n",
        "3.   Final Prediction: The final prediction is made by a weighted majority vote (for classification) or a weighted sum (for regression) of all weak learners. Learners with higher accuracy contribute more to the final decision.\n",
        "\n",
        "\n",
        "4.   Error Correction Mechanism: AdaBoost corrects errors by adjusting the weights of the training data points. It doesn't directly try to minimize a loss function like Gradient Boosting.\n",
        "\n",
        "\n",
        "**Gradient Boosting (e.g., GBM, XGBoost, LightGBM):**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Focus on Residuals (Errors): Gradient Boosting builds new weak learners to predict the residuals (the errors) of the previous predictors, rather than directly predicting the target variable or re-weighting data points.\n",
        "\n",
        "2.   Gradient Descent Optimization: It uses the concept of gradient descent to minimize a loss function. In each iteration, a weak learner is trained to predict the negative gradient of the loss function with respect to the current ensemble's prediction.\n",
        "3.   Sequential Addition: Each new weak learner is added to the ensemble, and its predictions are summed up with the predictions of previous learners.\n",
        "\n",
        "\n",
        "4.   Flexible Loss Functions: Gradient Boosting can optimize various differentiable loss functions (e.g., squared error for regression, log loss for classification), making it very flexible.\n",
        "\n",
        "\n",
        "5.   Learning Rate (Shrinkage): A learning rate (or shrinkage) is often applied to the contribution of each new tree to prevent overfitting and improve generalization.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-J_jv6XUL15b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "Answer:Regularization is a crucial aspect of XGBoost that helps to prevent overfitting and improve the model's generalization capabilities. XGBoost incorporates several regularization techniques, primarily through its objective function and tree-building process.\n",
        "\n",
        "Here's how regularization helps in XGBoost:\n",
        "\n",
        "\n",
        "\n",
        "1.   L1 (Lasso) and L2 (Ridge) Regularization on Weights (reg_alpha and reg_lambda):\n",
        "\n",
        "\n",
        "\n",
        "*   Concept: These penalties are added to the objective function to penalize large coefficients (weights) assigned to the leaves of the trees. L1 regularization (reg_alpha) adds the absolute value of the weights, encouraging sparsity (some feature weights become exactly zero, effectively performing feature selection).\n",
        "*   Benefit: By penalizing complex models with large weights, these regularizers prevent individual trees from becoming too specialized to the training data. This makes the model less sensitive to noise and specific training examples, leading to better performance on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "2.   Tree Pruning (gamma or min_split_loss):\n",
        "\n",
        "\n",
        "\n",
        "*   Concept: XGBoost uses a gamma parameter (also known as min_split_loss) to control tree pruning. A node is split only if the loss reduction after making the split is greater than gamma.\n",
        "*   Benefit: This prevents the growth of very deep and complex trees that might perfectly fit the training data but fail to generalize. By requiring a minimum gain to make a split, gamma effectively prunes less important branches, leading to simpler, more robust trees.\n",
        "\n",
        "\n",
        "3.   Maximum Tree Depth (max_depth):\n",
        "\n",
        "\n",
        "\n",
        "* Concept: This parameter directly limits how deep each individual tree in the ensemble can grow.\n",
        "*   Benefit: Limiting the tree depth restricts the complexity of each weak learner. Deeper trees can capture more specific patterns but are more prone to overfitting. By controlling max_depth, XGBoost prevents trees from memorizing the training data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "4.  Subsample Ratio of Training Instance (subsample):\n",
        "\n",
        "\n",
        "\n",
        "  * Concept: This parameter specifies the fraction of the training data to be randomly sampled for building each tree. It's similar to bagging but applied to individual trees within the boosting process.\n",
        "*   Benefit: Randomly sampling a subset of the data for each tree introduces diversity and reduces variance. It makes the model more robust by ensuring that each tree doesn't see the entire dataset, thus preventing over-reliance on specific data points and reducing overfitting.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "koxNa9uqM_9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "Answer:CatBoost (Categorical Boosting) is specifically designed to handle categorical features efficiently and effectively, distinguishing itself from other gradient boosting algorithms. Its efficiency stems from several innovative techniques:\n",
        "\n",
        "\n",
        "\n",
        "1.  **Ordered Target Encoding (Permutation-driven Alternative to Standard Target Encoding)**:\n",
        "\n",
        "\n",
        "\n",
        "* Problem with Standard Target Encoding: Traditional target encoding (replacing a categorical feature with the average of the target variable for that category) can suffer from target leakage. If all data is used to calculate the average, the model might learn specific noise from the target variable, leading to overfitting.\n",
        "*   CatBoost's Solution: CatBoost uses an 'ordered' or 'permutation-driven' target encoding approach. For each data point, it calculates the target mean for a categorical feature using only the previous data points in a random permutation. This prevents target leakage and makes the encoding more robust. It essentially creates multiple permutations of the dataset and calculates statistics for each feature based on the history of observations.\n",
        "\n",
        "*   Benefit: This method prevents overfitting that often occurs with standard target encoding, leading to better generalization and more stable models.\n",
        "\n",
        "\n",
        "\n",
        "2.   **Feature Combination (One-hot Encoding and Greedy Combinations):**\n",
        "\n",
        "\n",
        "\n",
        "* Concept: CatBoost can automatically combine categorical features to create new, more expressive features. It starts with simple combinations (e.g., combining two categorical features) and then greedily considers more complex combinations if they improve the model.\n",
        "*   Benefit: This helps capture interactions between different categorical features that might be missed if they were treated independently. For example, the combination of 'city=New York' and 'product=laptop' might have a different impact than either feature alone. This reduces the need for manual feature engineering of interactions.\n",
        "\n",
        "\n",
        "3.**Handling of Missing Categorical Values:**\n",
        "\n",
        "*Concept: CatBoost automatically handles missing values in categorical features. It treats NaNs as a separate, distinct category, allowing the model to learn patterns associated with missingness\n",
        "*Benefit: This eliminates the need for manual imputation strategies for categorical data, which can sometimes introduce bias or require complex preprocessing.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "__RoW3o4Oy4u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What are some real-world applications where boosting techniques are\n",
        "preferred over bagging methods?\n",
        "\n",
        "Answer:Boosting techniques are often preferred over bagging methods in scenarios where the focus is on achieving very high predictive accuracy, especially when dealing with complex datasets and when minimizing bias is crucial. Here are some real-world applications where boosting methods like XGBoost, LightGBM, and CatBoost often outperform bagging methods like Random Forests:\n",
        "\n",
        "\n",
        "\n",
        "1.   Fraud Detection: Fraudulent transactions are typically rare events (imbalanced data), and accurately identifying them is critical. Boosting algorithms, particularly those with strong regularization, can learn complex patterns from the minority class (fraudulent transactions) and assign higher weights to misclassified fraud instances, leading to better recall and precision\n",
        "\n",
        "2.   Credit Scoring and Risk Assessment: In finance, predicting the likelihood of loan default or credit risk requires highly accurate models. Boosting can capture intricate relationships between various financial indicators and customer behaviors, providing more precise risk scores than simpler models.\n",
        "\n",
        "3.   Customer Churn Prediction: Identifying customers likely to churn allows businesses to intervene proactively. Boosting models can effectively learn from historical customer data (usage patterns, demographics, interactions) to predict churn with high accuracy, even when the factors contributing to churn are subtle and interact in complex ways.\n",
        "\n",
        "\n",
        "4.   Ad Click-Through Rate (CTR) Prediction: In digital advertising, predicting whether a user will click on an ad is crucial for optimizing ad placement and revenue. Boosting algorithms excel here because they can model the complex interplay between user features, ad features, and context to predict CTR with high precision, which directly impacts advertising efficiency.\n",
        "\n"
      ],
      "metadata": {
        "id": "nuhwx5mCP41C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy"
      ],
      "metadata": {
        "id": "8s3h3AgYQjkG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e73e6b26",
        "outputId": "c250f075-20eb-4123-e000-1442227f900d"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost Classifier\n",
        "adaboost_model = AdaBoostClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "adaboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy on Breast Cancer dataset: {accuracy:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy on Breast Cancer dataset: 0.9708\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7:  Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score"
      ],
      "metadata": {
        "id": "dVOwn4dMQ28p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d94ba8",
        "outputId": "c921b3df-8b6c-4490-800b-258064cead6f"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gbr_model = GradientBoostingRegressor(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "gbr_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_gbr = gbr_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred_gbr)\n",
        "print(f\"Gradient Boosting Regressor R-squared score on California Housing dataset: {r2:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score on California Housing dataset: 0.7803\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy"
      ],
      "metadata": {
        "id": "gwHfa17_RCDj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71d41f39",
        "outputId": "147a3ab1-f408-48ae-c00f-47bb7e6f54f8"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost Classifier\n",
        "xgb_model = XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning_rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid,\n",
        "                           scoring='accuracy', cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Perform GridSearchCV to find the best learning rate\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Get the best estimator and evaluate its performance on the test set\n",
        "best_xgb_model = grid_search.best_estimator_\n",
        "y_pred_xgb = best_xgb_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy on the test set with the best parameters\n",
        "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
        "print(f\"XGBoost Classifier Accuracy on Breast Cancer dataset with best parameters: {accuracy_xgb:.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
            "Best parameters: {'learning_rate': 0.3}\n",
            "Best cross-validation accuracy: 0.9674\n",
            "XGBoost Classifier Accuracy on Breast Cancer dataset with best parameters: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn"
      ],
      "metadata": {
        "id": "QCir8eKHRa-C"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bc127066",
        "outputId": "12fddf16-4bda-43f9-f00e-7f2aec4b21b8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Install catboost if not already installed\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the CatBoost Classifier\n",
        "# verbose=0 to suppress training output, random_seed for reproducibility\n",
        "cat_model = CatBoostClassifier(iterations=100,  # Number of boosting iterations\n",
        "                               learning_rate=0.1,\n",
        "                               depth=6,         # Depth of the trees\n",
        "                               loss_function='Logloss', # For binary classification\n",
        "                               eval_metric='Accuracy',  # Metric to evaluate during training\n",
        "                               random_seed=42,\n",
        "                               verbose=0)       # Suppress verbose output\n",
        "\n",
        "# Train the model\n",
        "print(\"Training CatBoost Classifier...\")\n",
        "cat_model.fit(X_train, y_train)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_cat = cat_model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_cat = accuracy_score(y_test, y_pred_cat)\n",
        "print(f\"CatBoost Classifier Accuracy on Breast Cancer dataset: {accuracy_cat:.4f}\")\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_cat)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=breast_cancer.target_names,\n",
        "            yticklabels=breast_cancer.target_names)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix for CatBoost Classifier (Breast Cancer Dataset)')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.3.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (9.1.2)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "Training CatBoost Classifier...\n",
            "Training complete.\n",
            "CatBoost Classifier Accuracy on Breast Cancer dataset: 0.9708\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model"
      ],
      "metadata": {
        "id": "vF6L5yJYRdf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13dae0e0"
      },
      "source": [
        "# Task\n",
        "Develop a comprehensive data science pipeline for predicting loan defaults for a FinTech company using an appropriate loan default prediction dataset. The pipeline should include: robust data preprocessing, strategies for handling missing values and categorical features (considering potential class imbalance), selection and justification of a boosting algorithm (AdaBoost, XGBoost, or CatBoost) with a detailed hyperparameter tuning strategy, and evaluation using appropriate metrics for imbalanced classification (e.g., Precision, Recall, F1-score, AUC-ROC, confusion matrix). Finally, describe the business benefits this model would provide to the FinTech company."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b6b94db"
      },
      "source": [
        "## Data Preprocessing & Handling Missing/Categorical Values\n",
        "\n",
        "### Subtask:\n",
        "Detail the steps for data preprocessing, including techniques for handling missing values (e.g., imputation strategies like mean/median/mode imputation for numerical features and specific strategies for categorical features) and encoding categorical features (e.g., one-hot encoding for features with fewer categories, target encoding or CatBoost's ordered encoding for features with many categories). Also, describe methods to address class imbalance (e.g., SMOTE, undersampling, or using appropriate loss functions/metrics).\n",
        "\n",
        "---\n",
        "\n",
        "### 1. Initial Data Loading and Inspection\n",
        "\n",
        "To begin, we'll load the loan default prediction dataset. This initial step is crucial for understanding the raw data's characteristics. We'll use libraries like `pandas` to load the data into a DataFrame. Once loaded, we'll perform several inspections:\n",
        "\n",
        "*   **Structure (`df.info()`):** Check column names, data types (e.g., int64, float64, object), and non-null counts to identify potential missing values at a high level.\n",
        "*   **Summary Statistics (`df.describe()`):** Obtain descriptive statistics for numerical features (mean, median, standard deviation, min, max, quartiles) to understand their distribution and range. This helps in identifying outliers and informs imputation strategies.\n",
        "*   **Head/Tail (`df.head()`, `df.tail()`):** View the first and last few rows of the DataFrame to get a glimpse of the actual data entries.\n",
        "*   **Unique Values and Counts (`df['column'].value_counts()`):** For categorical features, check the number of unique categories and their frequencies to gauge cardinality and identify potential issues like typos or rare categories.\n",
        "*   **Missing Value Count (`df.isnull().sum()`):** Accurately identify the number and percentage of missing values per column, which will guide our imputation strategies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e5b89df"
      },
      "source": [
        "### 2. Handling Missing Values\n",
        "\n",
        "Missing values are common in real-world datasets and need to be handled appropriately to prevent biases and errors in the model. The approach varies for numerical and categorical features:\n",
        "\n",
        "*   **Numerical Features:**\n",
        "    *   **Identification:** Identify numerical columns with missing values using `df.isnull().sum()` or similar methods.\n",
        "    *   **Imputation Strategies:**\n",
        "        *   **Mean Imputation:** Suitable for normally distributed data without significant outliers. Replaces missing values with the average of the observed values in that column. However, it can distort the distribution and reduce variance.\n",
        "        *   **Median Imputation:** Preferred for skewed distributions or when outliers are present, as the median is less sensitive to extreme values. It helps preserve the shape of the distribution better than mean imputation.\n",
        "        *   **Mode Imputation:** Less common for numerical data, but can be used if a specific value is overwhelmingly frequent.\n",
        "        *   **Advanced Imputation (e.g., KNN Imputation):** For more complex scenarios, K-Nearest Neighbors (KNN) imputation can be used, where missing values are imputed based on the values of their `k` nearest neighbors. This method considers the relationship between features but is computationally more intensive.\n",
        "    *   **Justification:** The choice between mean, median, or more advanced methods will depend on the distribution of each specific numerical feature and the overall impact on the model's performance and interpretability. For this FinTech context, preserving the distribution is crucial, so median imputation for skewed numerical features might be a strong candidate.\n",
        "\n",
        "*   **Categorical Features:**\n",
        "    *   **Identification:** Identify categorical columns with missing values.\n",
        "    *   **Treatment Options:**\n",
        "        *   **Treat as a Separate Category:** This is often the safest and most common approach. Missing values are assigned a new category, e.g., 'Unknown' or 'Missing'. This allows the model to learn if missingness itself carries predictive information.\n",
        "        *   **Mode Imputation:** Replace missing values with the most frequent category (mode) in that column. This can be used if the percentage of missing values is small and the mode is clearly dominant.\n",
        "        *   **Justification:** Treating missing values as a separate category is generally preferred as it doesn't assume that missingness is random and allows the model to capture patterns associated with missing data. This is particularly relevant in financial datasets where missing information might be indicative of specific customer behavior or data collection issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26f8557"
      },
      "source": [
        "### 3. Encoding Categorical Features\n",
        "\n",
        "Categorical features need to be converted into a numerical format that machine learning algorithms can process. The choice of encoding depends heavily on the cardinality (number of unique categories) of the feature and the type of boosting algorithm being used.\n",
        "\n",
        "*   **Low Cardinality Categorical Features (e.g., gender, loan type, marital status):**\n",
        "    *   **One-Hot Encoding:** This is a common and effective method for features with a small number of unique categories. It creates a new binary column for each unique category, where a '1' indicates the presence of that category and '0' indicates its absence. This prevents the model from assuming an ordinal relationship between categories. Libraries like `pandas.get_dummies()` or `sklearn.preprocessing.OneHotEncoder` are suitable.\n",
        "    *   **Benefit:** Prevents the model from misinterpreting arbitrary numerical assignments as ordinal relationships. Simple and widely understood.\n",
        "    *   **Drawback:** Can lead to a high-dimensional feature space if there are many low-cardinality features or a single feature with moderately many categories, which can slow down training and increase memory usage.\n",
        "\n",
        "*   **High Cardinality Categorical Features (e.g., ZIP codes, customer IDs, product names):**\n",
        "    *   **Problem with One-Hot Encoding:** For features with many unique categories, one-hot encoding can lead to a very sparse and high-dimensional dataset, which can cause the curse of dimensionality, increase training time, and potentially lead to overfitting.\n",
        "    *   **Target Encoding (Mean Encoding):** This technique replaces each category with the mean of the target variable for that category. For example, for a ZIP code, it would be replaced by the average default rate for that ZIP code. It significantly reduces dimensionality.\n",
        "        *   **Precautions (to prevent Target Leakage):** To avoid target leakage (where information from the target variable influences the feature during training), target encoding must be applied carefully. This typically involves using cross-validation (e.g., K-fold target encoding) or training data only to compute the means, and then applying those means to the validation/test sets. Another robust method is **smoothing**, which blends the category mean with the global target mean based on the number of samples in the category.\n",
        "    *   **CatBoost's Ordered Target Encoding (Built-in):** If CatBoost is chosen as the boosting algorithm, it offers an efficient and robust built-in mechanism to handle categorical features, including high-cardinality ones. CatBoost employs an 'ordered' target encoding approach:\n",
        "        *   For each data point, the target mean for a categorical feature is calculated using only data points observed *before* the current one in a random permutation of the dataset. This effectively prevents target leakage and makes the encoding more robust.\n",
        "        *   It can also handle combinations of categorical features automatically.\n",
        "        *   **Benefit:** Eliminates the need for manual categorical encoding and prevents overfitting commonly associated with standard target encoding, leading to more stable and accurate models.\n",
        "    *   **Justification:** For a FinTech loan default prediction model, high-cardinality features like transaction types, merchant IDs, or specific demographic sub-categories are likely. CatBoost's ordered target encoding offers a superior and more robust solution by inherently preventing target leakage and handling feature interactions, making it a strong candidate if CatBoost is the chosen model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10a01d30"
      },
      "source": [
        "### 4. Addressing Class Imbalance\n",
        "\n",
        "Loan default prediction datasets are typically highly imbalanced, with a significantly smaller number of default cases (minority class) compared to non-default cases (majority class). Ignoring this imbalance can lead to models that perform poorly on the minority class, as they tend to optimize for overall accuracy, which is often dominated by the majority class. Addressing class imbalance is crucial for building effective fraud detection models.\n",
        "\n",
        "*   **Analysis of Class Distribution:**\n",
        "    *   First, we need to quantify the imbalance by checking the value counts of the target variable (`y.value_counts()`) and visualizing it (e.g., using a bar plot or pie chart) to clearly see the disproportion.\n",
        "\n",
        "*   **Strategies to Mitigate Imbalance:**\n",
        "    *   **Oversampling the Minority Class (e.g., SMOTE - Synthetic Minority Over-sampling Technique):**\n",
        "        *   **Concept:** SMOTE generates synthetic samples for the minority class. It works by selecting a minority class instance and finding its k-nearest neighbors. New synthetic instances are then created by interpolating between the selected instance and its neighbors.\n",
        "        *   **Benefit:** Increases the number of minority samples, helping the model learn their patterns more effectively without losing information from the majority class.\n",
        "        *   **Drawback:** Can introduce noise if not applied carefully, and the synthetic samples might not perfectly represent real-world minority instances. Overfitting to the synthetic samples is a risk.\n",
        "    *   **Undersampling the Majority Class:**\n",
        "        *   **Concept:** Reduces the number of samples in the majority class to balance the class distribution. This can be done randomly or by more sophisticated methods (e.g., NearMiss, Tomek Links) that remove majority samples that are close to the decision boundary.\n",
        "        *   **Benefit:** Reduces training time and memory requirements, can help focus the model on the minority class.\n",
        "        *   **Drawback:** Can lead to a significant loss of potentially useful information from the majority class, which might reduce the model's overall predictive power.\n",
        "    *   **Combined Sampling (e.g., SMOTE + ENN):**\n",
        "        *   **Concept:** Combines oversampling of the minority class with undersampling of the majority class to achieve a more balanced and cleaner dataset.\n",
        "        *   **Benefit:** Often more effective than using either oversampling or undersampling alone.\n",
        "    *   **Algorithm-Level Approaches:**\n",
        "        *   **Cost-Sensitive Learning (Adjusting Class Weights):** Many boosting algorithms (like XGBoost, LightGBM, CatBoost) allow specifying `scale_pos_weight` or `class_weights`. This assigns a higher penalty to misclassifications of the minority class, forcing the model to pay more attention to them. This is often the preferred method with boosting, as it avoids altering the actual data distribution.\n",
        "        *   **Using Appropriate Evaluation Metrics:** Instead of solely relying on accuracy, use metrics that are more robust to imbalance, such as Precision, Recall, F1-Score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR). These metrics provide a more nuanced view of the model's performance on both classes.\n",
        "\n",
        "*   **Justification:** For loan default prediction, misclassifying a defaulting customer as non-defaulting (false negative) can be more costly than misclassifying a non-defaulting customer as defaulting (false positive). Therefore, prioritizing **Recall** for the minority class (default) and **Precision** should be carefully balanced. Using class weights directly within the boosting algorithm is often a very effective and clean approach, as it modifies the learning process without changing the underlying data, making it less prone to issues like data leakage or artificial patterns introduced by sampling methods. Combining this with robust evaluation metrics is key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541af652"
      },
      "source": [
        "### 4. Addressing Class Imbalance\n",
        "\n",
        "Loan default prediction datasets are typically highly imbalanced, with a significantly smaller number of default cases (minority class) compared to non-default cases (majority class). Ignoring this imbalance can lead to models that perform poorly on the minority class, as they tend to optimize for overall accuracy, which is often dominated by the majority class. Addressing class imbalance is crucial for building effective fraud detection models.\n",
        "\n",
        "*   **Analysis of Class Distribution:**\n",
        "    *   First, we need to quantify the imbalance by checking the value counts of the target variable (`y.value_counts()`) and visualizing it (e.g., using a bar plot or pie chart) to clearly see the disproportion.\n",
        "\n",
        "*   **Strategies to Mitigate Imbalance:**\n",
        "    *   **Oversampling the Minority Class (e.g., SMOTE - Synthetic Minority Over-sampling Technique):**\n",
        "        *   **Concept:** SMOTE generates synthetic samples for the minority class. It works by selecting a minority class instance and finding its k-nearest neighbors. New synthetic instances are then created by interpolating between the selected instance and its neighbors.\n",
        "        *   **Benefit:** Increases the number of minority samples, helping the model learn their patterns more effectively without losing information from the majority class.\n",
        "        *   **Drawback:** Can introduce noise if not applied carefully, and the synthetic samples might not perfectly represent real-world minority instances. Overfitting to the synthetic samples is a risk.\n",
        "    *   **Undersampling the Majority Class:**\n",
        "        *   **Concept:** Reduces the number of samples in the majority class to balance the class distribution. This can be done randomly or by more sophisticated methods (e.g., NearMiss, Tomek Links) that remove majority samples that are close to the decision boundary.\n",
        "        *   **Benefit:** Reduces training time and memory requirements, can help focus the model on the minority class.\n",
        "        *   **Drawback:** Can lead to a significant loss of potentially useful information from the majority class, which might reduce the model's overall predictive power.\n",
        "    *   **Combined Sampling (e.g., SMOTE + ENN):**\n",
        "        *   **Concept:** Combines oversampling of the minority class with undersampling of the majority class to achieve a more balanced and cleaner dataset.\n",
        "        *   **Benefit:** Often more effective than using either oversampling or undersampling alone.\n",
        "    *   **Algorithm-Level Approaches:**\n",
        "        *   **Cost-Sensitive Learning (Adjusting Class Weights):** Many boosting algorithms (like XGBoost, LightGBM, CatBoost) allow specifying `scale_pos_weight` or `class_weights`. This assigns a higher penalty to misclassifications of the minority class, forcing the model to pay more attention to them. This is often the preferred method with boosting, as it avoids altering the actual data distribution.\n",
        "        *   **Using Appropriate Evaluation Metrics:** Instead of solely relying on accuracy, use metrics that are more robust to imbalance, such as Precision, Recall, F1-Score, Area Under the Receiver Operating Characteristic Curve (AUC-ROC), and Area Under the Precision-Recall Curve (AUC-PR). These metrics provide a more nuanced view of the model's performance on both classes.\n",
        "\n",
        "*   **Justification:** For loan default prediction, misclassifying a defaulting customer as non-defaulting (false negative) can be more costly than misclassifying a non-defaulting customer as defaulting (false positive). Therefore, prioritizing **Recall** for the minority class (default) and **Precision** should be carefully balanced. Using class weights directly within the boosting algorithm is often a very effective and clean approach, as it modifies the learning process without changing the underlying data, making it less prone to issues like data leakage or artificial patterns introduced by sampling methods. Combining this with robust evaluation metrics is key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d4f424a"
      },
      "source": [
        "## Choice of Boosting Algorithm\n",
        "\n",
        "### Subtask:\n",
        "Discuss the strengths and weaknesses of AdaBoost, XGBoost, and CatBoost in the context of the given problem (imbalanced data, missing values, numeric and categorical features). Justify the selection of the most suitable boosting algorithm(s) for this specific scenario, highlighting features like CatBoost's native categorical feature handling or XGBoost's flexibility and regularization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba48424a"
      },
      "source": [
        "## Choice of Boosting Algorithm\n",
        "\n",
        "For predicting loan default in a FinTech company, we face several key data characteristics:\n",
        "*   **Imbalanced Data**: Loan defaults are typically rare events compared to non-defaults.\n",
        "*   **Missing Values**: Common in real-world financial datasets.\n",
        "*   **Numeric and Categorical Features**: Customer demographics (e.g., age, income, education) are often numeric, while transaction behaviors or certain financial indicators can be categorical (e.g., credit score tier, payment frequency categories).\n",
        "\n",
        "Let's analyze the strengths and weaknesses of AdaBoost, XGBoost, and CatBoost in this context:\n",
        "\n",
        "### 1. AdaBoost (Adaptive Boosting)\n",
        "\n",
        "**Strengths:**\n",
        "*   **Simple to understand and implement.**\n",
        "*   **Effective for binary classification:** The loan default problem is a binary classification task.\n",
        "*   **Focuses on misclassified samples:** Iteratively assigns higher weights to misclassified instances, which can be beneficial for imbalanced datasets if the minority class is the one being misclassified. This inherent mechanism helps in giving more attention to the 'difficult' (often minority class) examples.\n",
        "\n",
        "**Weaknesses:**\n",
        "*   **Sensitive to noisy data and outliers:** Since it continuously tries to correct previous errors, outliers can get disproportionately high weights, leading to overfitting or poor generalization.\n",
        "*   **Less robust with complex features:** Does not natively handle missing values or categorical features, requiring extensive preprocessing (imputation, one-hot encoding).\n",
        "*   **Not explicitly designed for severe class imbalance:** While it focuses on errors, extreme imbalance can still lead to the model being overwhelmed by the majority class, requiring additional techniques like resampling.\n",
        "\n",
        "### 2. XGBoost (Extreme Gradient Boosting)\n",
        "\n",
        "**Strengths:**\n",
        "*   **Highly Flexible and Scalable:** Can optimize various objective functions and loss functions, making it versatile for different problem types.\n",
        "*   **Robust Regularization:** Incorporates L1 (Lasso) and L2 (Ridge) regularization, tree pruning (gamma), maximum tree depth, and subsampling (column and row) to prevent overfitting, which is crucial for complex FinTech data.\n",
        "*   **Native Handling of Missing Values:** XGBoost has a sparse-aware split finding algorithm that can automatically learn the best direction for splits when a value is missing, without requiring explicit imputation.\n",
        "*   **Excellent Performance:** Often achieves state-of-the-art results in competitive machine learning tasks due to its optimized algorithms and parallel processing capabilities.\n",
        "\n",
        "**Weaknesses:**\n",
        "*   **Requires extensive preprocessing for categorical features:** Does not handle categorical features natively. They must be encoded (e.g., one-hot encoding, label encoding, target encoding) before feeding to XGBoost. Target encoding can be susceptible to target leakage if not implemented carefully.\n",
        "*   **Can be prone to overfitting:** Despite its regularization features, careful tuning of hyperparameters is still necessary, especially with deep trees and a high learning rate.\n",
        "\n",
        "### 3. CatBoost (Categorical Boosting)\n",
        "\n",
        "**Strengths:**\n",
        "*   **Native Handling of Categorical Features:** CatBoost employs a sophisticated technique called Ordered Target Encoding (permutation-driven target encoding) to convert categorical features into numerical ones without target leakage. This is a significant advantage, reducing preprocessing effort and risk of overfitting.\n",
        "*   **Robust Handling of Missing Values:** Automatically treats `NaN` as a separate category, similar to how it handles other feature values, reducing the need for manual imputation.\n",
        "*   **Strong Regularization:** Uses a symmetric tree structure and ordered boosting to combat overfitting, leading to more robust models.\n",
        "*   **Good Performance:** Often competitive with XGBoost and LightGBM, particularly on datasets with a high number of categorical features.\n",
        "*   **Reduced Hyperparameter Tuning for Categoricals:** Since it handles categoricals internally, the user has fewer categorical-related hyperparameters to tune.\n",
        "\n",
        "**Weaknesses:**\n",
        "*   **Can be slower to train:** The ordered target encoding and ordered boosting scheme can make training slower compared to other algorithms like LightGBM, especially on large datasets with many categorical features.\n",
        "*   **Higher memory consumption:** The internal representation and handling of categorical features can consume more memory.\n",
        "*   **Less widespread community support:** While growing, its community and resources are smaller compared to XGBoost.\n",
        "\n",
        "### Justification for Selection\n",
        "\n",
        "For loan default prediction in a FinTech context, considering the characteristics of the problem (imbalanced data, missing values, and a mix of numeric and categorical features), **CatBoost** and **XGBoost** are the most suitable algorithms, with a slight preference for **CatBoost** given the native handling of categorical features.\n",
        "\n",
        "**Why CatBoost is highly suitable:**\n",
        "1.  **Native Categorical Feature Handling:** This is a major advantage. FinTech datasets often have numerous categorical features, and CatBoost's Ordered Target Encoding eliminates the need for manual and often risky encoding methods (like standard target encoding which can lead to target leakage) or computationally expensive one-hot encoding for high-cardinality features. This simplifies the preprocessing pipeline and builds a more robust model.\n",
        "2.  **Robust Missing Value Handling:** Automatically treating `NaN`s as a distinct category is beneficial and reduces the need for complex imputation strategies, which can be challenging and prone to errors in a FinTech context.\n",
        "3.  **Resistance to Overfitting:** Its internal mechanisms for regularization (symmetric trees, ordered boosting) are well-suited for noisy, real-world data.\n",
        "4.  **Good performance on imbalanced data:** While not explicitly designed for imbalanced data, its robust nature and focus on learning subtle patterns can perform well. We can further enhance this with techniques like `scale_pos_weight` or `class_weights` parameters, or external resampling methods.\n",
        "\n",
        "**Why XGBoost is also a strong candidate:**\n",
        "*   **Flexibility and Performance:** XGBoost's power lies in its extreme flexibility and consistent high performance across various domains. Its regularization capabilities are excellent for preventing overfitting.\n",
        "*   **Native Missing Value Handling:** This is a crucial benefit, as financial data often contains missing information.\n",
        "*   **Extensive Community Support:** A vast community and abundant resources make debugging and implementation easier.\n",
        "\n",
        "**Recommendation:**\n",
        "We would primarily recommend **CatBoost** due to its superior handling of categorical features and missing values, which are prevalent in the given problem description. It simplifies the data preprocessing pipeline and inherently reduces the risk of target leakage, leading to more generalizable models. If, however, CatBoost proves to be too slow or memory-intensive for the specific dataset size, or if we have a robust and leakage-free categorical encoding strategy in place, **XGBoost** would be an excellent alternative, offering similar levels of predictive power with its robust regularization and missing value handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d76bec6"
      },
      "source": [
        "## Hyperparameter Tuning Strategy\n",
        "\n",
        "### Subtask:\n",
        "Outline a robust hyperparameter tuning strategy, including methods like GridSearchCV or RandomizedSearchCV, defining a search space for key hyperparameters, and cross-validation techniques.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99b6da97"
      },
      "source": [
        "## Hyperparameter Tuning Strategy\n",
        "\n",
        "### 1. Importance of Hyperparameter Tuning for Boosting Models\n",
        "\n",
        "Hyperparameter tuning is crucial for boosting models, especially in high-stakes applications like loan default prediction. Boosting algorithms, by their sequential nature, are powerful but also prone to overfitting the training data if not properly constrained. In the context of loan default, where the target variable (default/non-default) is typically highly imbalanced, tuning becomes even more critical.\n",
        "\n",
        "*   **Preventing Overfitting**: Untuned boosting models can easily memorize noise in the training data, leading to excellent performance on the training set but poor generalization to new, unseen data. Tuning parameters like `max_depth`, `learning_rate`, `n_estimators`, and regularization terms (`reg_alpha`, `reg_lambda`, `l2_leaf_reg`) allows us to control model complexity and prevent it from becoming too specialized.\n",
        "*   **Improving Generalization**: A well-tuned model strikes a balance between bias and variance, leading to more robust predictions on future loan applications. This means the model can accurately identify potential defaulters while minimizing false positives (rejecting creditworthy applicants) and false negatives (approving high-risk applicants).\n",
        "*   **Handling Imbalance**: Parameters related to class weights (`scale_pos_weight`, `class_weights`) are vital in imbalanced datasets to ensure the model doesn't simply predict the majority class. Proper tuning helps the model learn the characteristics of the minority class (loan defaulters) effectively.\n",
        "*   **Optimizing Performance**: Different datasets and business problems require different model configurations. Hyperparameter tuning systematically explores these configurations to find the optimal set that maximizes the desired performance metric for the specific task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26f47102"
      },
      "source": [
        "### 2. Hyperparameter Tuning Methods: GridSearchCV vs. RandomizedSearchCV\n",
        "\n",
        "To systematically search for the best combination of hyperparameters, we can employ automated search techniques:\n",
        "\n",
        "*   **GridSearchCV**: This method performs an exhaustive search over all possible combinations of hyperparameters specified in a grid. For every combination, it trains the model with K-fold cross-validation and evaluates its performance using a predefined scoring metric.\n",
        "    *   **When to Use**: Ideal for smaller search spaces or when you want to ensure that every possible combination within the defined range is tested. It's suitable when computational resources allow for thorough exploration of the hyperparameter landscape.\n",
        "    *   **Pros**: Guarantees finding the best combination within the defined grid.\n",
        "    *   **Cons**: Can be computationally very expensive and time-consuming, especially with a large number of hyperparameters or a wide range of values for each, as the number of models to train grows exponentially.\n",
        "\n",
        "*   **RandomizedSearchCV**: Instead of trying every combination, RandomizedSearchCV samples a fixed number of parameter settings from specified distributions. It performs K-fold cross-validation for each sampled combination. This approach allows for a more efficient search in large hyperparameter spaces.\n",
        "    *   **When to Use**: Preferred for larger search spaces or when computational budget is limited. It's generally more efficient than GridSearchCV for complex models with many hyperparameters, as it often finds a good (though not necessarily the absolute best) set of hyperparameters much faster.\n",
        "    *   **Pros**: Much faster than GridSearchCV for large search spaces. Often finds very good results in a fraction of the time.\n",
        "    *   **Cons**: Does not guarantee finding the absolute best combination as it doesn't exhaustively search the entire space. The quality of the results depends on the number of iterations (`n_iter`) and the chosen distributions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "617a48ec"
      },
      "source": [
        "### 3. Defining a Search Space for Key Hyperparameters\n",
        "\n",
        "The choice of boosting algorithm (XGBoost or CatBoost) will influence the exact parameter names, but many concepts are shared. Here's a typical search space for key hyperparameters, keeping in mind the need to balance model complexity and performance on an imbalanced dataset:\n",
        "\n",
        "#### For XGBoost:\n",
        "\n",
        "*   `learning_rate` (or `eta`): Controls the step size shrinkage. Smaller values make the boosting process more robust to overfitting but require more `n_estimators`.\n",
        "    *   **Search Space**: `[0.01, 0.05, 0.1, 0.2, 0.3]`\n",
        "*   `n_estimators`: The number of boosting rounds or trees to build.\n",
        "    *   **Search Space**: `[100, 200, 300, 500, 700]` (This often needs to be tuned in conjunction with `learning_rate`)\n",
        "*   `max_depth`: Maximum depth of a tree. Higher values can lead to overfitting.\n",
        "    *   **Search Space**: `[3, 5, 7, 9]`\n",
        "*   `subsample`: Fraction of samples used for fitting the individual base learners.\n",
        "    *   **Search Space**: `[0.6, 0.7, 0.8, 0.9, 1.0]`\n",
        "*   `colsample_bytree`: Fraction of features (columns) used for fitting the individual base learners.\n",
        "    *   **Search Space**: `[0.6, 0.7, 0.8, 0.9, 1.0]`\n",
        "*   `gamma` (or `min_split_loss`): Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
        "    *   **Search Space**: `[0, 0.1, 0.2, 0.3]`\n",
        "*   `reg_alpha` (L1 regularization term on weights): Helps prevent overfitting by shrinking leaf weights towards zero.\n",
        "    *   **Search Space**: `[0, 0.001, 0.01, 0.1, 1]`\n",
        "*   `reg_lambda` (L2 regularization term on weights): Another regularization term to prevent overfitting.\n",
        "    *   **Search Space**: `[0, 0.001, 0.01, 0.1, 1]`\n",
        "*   `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child.\n",
        "    *   **Search Space**: `[1, 5, 10]`\n",
        "*   `scale_pos_weight`: Controls the balance of positive and negative weights, useful for imbalanced classes.\n",
        "    *   **Search Space**: Calculated based on the inverse ratio of negative to positive samples: `(count(negative_examples) / count(positive_examples))`\n",
        "\n",
        "#### For CatBoost:\n",
        "\n",
        "*   `iterations` (similar to `n_estimators`): The number of boosting iterations.\n",
        "    *   **Search Space**: `[100, 200, 300, 500, 700]`\n",
        "*   `learning_rate`: Step size shrinkage.\n",
        "    *   **Search Space**: `[0.01, 0.05, 0.1, 0.2, 0.3]`\n",
        "*   `depth` (similar to `max_depth`): Depth of the trees.\n",
        "    *   **Search Space**: `[3, 5, 7, 9]`\n",
        "*   `l2_leaf_reg`: L2 regularization coefficient.\n",
        "    *   **Search Space**: `[1, 3, 5, 7, 9]`\n",
        "*   `subsample`: Fraction of samples to be used for training each tree.\n",
        "    *   **Search Space**: `[0.6, 0.7, 0.8, 0.9, 1.0]`\n",
        "*   `colsample_bylevel` (similar to `colsample_bytree`): Fraction of features to be considered for each split.\n",
        "    *   **Search Space**: `[0.6, 0.7, 0.8, 0.9, 1.0]`\n",
        "*   `min_data_in_leaf`: Minimum number of samples in a leaf.\n",
        "    *   **Search Space**: `[1, 5, 10]`\n",
        "*   `class_weights`: Weights for classes, useful for imbalanced datasets.\n",
        "    *   **Search Space**: `[1, ratio]` or `[0.5, 1.5]` etc. where ratio is `(count(negative_examples) / count(positive_examples))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d71ec5f"
      },
      "source": [
        "### 4. Cross-Validation Techniques\n",
        "\n",
        "Cross-validation is an essential part of hyperparameter tuning, especially with methods like `GridSearchCV` and `RandomizedSearchCV`. It ensures that the model's performance evaluation is robust and generalizable to unseen data.\n",
        "\n",
        "*   **K-Fold Cross-Validation**: The most common technique. The training dataset is split into `k` equally sized folds. The model is trained `k` times; each time, one fold is used as the validation set, and the remaining `k-1` folds are used for training. The performance metric (e.g., F1-score) is calculated for each of the `k` validation sets, and the average of these `k` scores is used as the overall performance estimate for that specific hyperparameter combination.\n",
        "    *   **Benefit**: This approach provides a more reliable estimate of the model's generalization performance than a single train-test split, as it reduces the variance associated with the random splitting of data. It ensures that every data point gets to be in a validation set exactly once.\n",
        "    *   **Consideration for Time-Series Data**: For datasets with a temporal component (e.g., transaction behavior over time), a simple K-fold might not be appropriate. In such cases, **Time-Series Cross-Validation** (or `TimeSeriesSplit` in scikit-learn) should be used, where the training sets consist of observations prior to the validation set. This simulates how the model would be used in a real-world scenario to predict future events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74bd7881"
      },
      "source": [
        "### 5. Evaluation Metrics for Hyperparameter Tuning\n",
        "\n",
        "For hyperparameter tuning in the context of loan default prediction, where the dataset is inherently imbalanced (far fewer default cases than non-default cases), using simple `accuracy` as a scoring metric is misleading. A model that predicts 'no default' for every instance could achieve high accuracy if the non-default class is dominant, but it would be useless for identifying actual defaulters.\n",
        "\n",
        "Therefore, we must choose **imbalance-aware metrics** that prioritize the correct identification of the minority class (loan defaults) and provide a more balanced view of the model's performance:\n",
        "\n",
        "*   **F1-Score (F1-weighted or F1-macro/micro)**: The F1-score is the harmonic mean of precision and recall. It balances both false positives and false negatives, making it suitable for imbalanced datasets. `f1_weighted` considers label imbalance by computing the average F1 score of each class weighted by their presence in the true data.\n",
        "\n",
        "*   **ROC AUC (Receiver Operating Characteristic Area Under the Curve)**: ROC AUC measures the ability of a classifier to distinguish between classes. It's robust to class imbalance because it considers all possible classification thresholds. A higher AUC indicates a better ability to separate positive from negative classes, which is crucial for identifying defaulters without being overly sensitive to the majority class.\n",
        "\n",
        "*   **Average Precision (AP) or Precision-Recall AUC**: This metric computes the average precision over the recall curve. It is particularly useful for highly imbalanced datasets where the minority class is of primary interest. Unlike ROC AUC, which can be overly optimistic for highly imbalanced data, AP focuses on the precision-recall trade-off and is more sensitive to false positives and false negatives involving the positive class. In scenarios where identifying most of the positive cases (high recall) while maintaining reasonable precision is critical (e.g., in fraud detection or loan default), AP can be a more informative metric than ROC AUC.\n",
        "\n",
        "When configuring `GridSearchCV` or `RandomizedSearchCV`, these metrics should be specified in the `scoring` parameter. For example, `scoring='f1'` or `scoring='roc_auc'`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a66d564"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "### Subtask:\n",
        "Identify and explain the appropriate evaluation metrics for a loan default prediction model, considering the imbalanced nature of the dataset. Metrics like Precision, Recall, F1-score, AUC-ROC curve, and a confusion matrix should be discussed, with a rationale for why each is important in this context.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c44dc40"
      },
      "source": [
        "### Evaluation Metrics for Loan Default Prediction\n",
        "\n",
        "When evaluating a loan default prediction model, standard accuracy is often insufficient and misleading, especially in datasets with inherent class imbalance. In financial contexts, loan defaults are typically rare events compared to non-defaults. If a model simply predicts 'no default' for every applicant, it might achieve very high accuracy (e.g., 95% if only 5% of loans default). However, such a model would be useless as it fails to identify any defaulters. More critically, the business implication of misclassifying a defaulter (a **False Negative**) by granting them a loan can lead to significant financial losses. Conversely, misclassifying a non-defaulter as a defaulter (a **False Positive**) can lead to wrongly denying a loan to a creditworthy customer, resulting in lost business opportunities and potential reputational damage. Given these asymmetric costs, a more nuanced set of evaluation metrics is essential.\n",
        "\n",
        "#### 1. Confusion Matrix\n",
        "The **Confusion Matrix** is a fundamental tool for understanding the performance of a classification model. It summarizes the predictions made by the model against the actual labels, allowing us to visualize the types of errors and correct predictions:\n",
        "\n",
        "*   **True Positives (TP)**: The model correctly predicted a loan default (actual default = True, predicted default = True). These are the actual defaulters identified by our model, which helps the FinTech company prevent losses.\n",
        "*   **True Negatives (TN)**: The model correctly predicted no loan default (actual default = False, predicted default = False). These are the creditworthy customers who are correctly identified.\n",
        "*   **False Positives (FP)**: The model incorrectly predicted a loan default (actual default = False, predicted default = True). In a loan context, this means rejecting a creditworthy applicant. This results in lost revenue and potentially a negative customer experience.\n",
        "*   **False Negatives (FN)**: The model incorrectly predicted no loan default (actual default = True, predicted default = False). This is a crucial error in loan default prediction: approving a loan for an applicant who will actually default. This directly leads to financial losses for the FinTech company.\n",
        "\n",
        "#### 2. Precision (Positive Predictive Value)\n",
        "**Precision** measures the proportion of positive identifications that were actually correct. It is calculated as:\n",
        "\n",
        "`Precision = TP / (TP + FP)`\n",
        "\n",
        "In the context of loan default prediction, a high precision indicates that when the model predicts someone will default, it is usually correct. This metric is important for minimizing **False Positives**. For a FinTech company, high precision means avoiding the costly mistake of wrongly denying loans to creditworthy customers, which could otherwise lead to customer dissatisfaction and loss of potential revenue.\n",
        "\n",
        "#### 3. Recall (Sensitivity)\n",
        "**Recall** measures the proportion of actual positives that were correctly identified. It is calculated as:\n",
        "\n",
        "`Recall = TP / (TP + FN)`\n",
        "\n",
        "For loan default prediction, high recall means the model is effective at identifying most of the actual defaulters. This metric is critically important for minimizing **False Negatives**, as failing to identify a defaulter can lead to significant financial losses for the company. Maximizing recall helps the FinTech company mitigate risks by catching as many potential defaulters as possible.\n",
        "\n",
        "#### 4. F1-Score\n",
        "The **F1-Score** is the harmonic mean of Precision and Recall. It provides a single score that balances both metrics:\n",
        "\n",
        "`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
        "\n",
        "The F1-score is particularly valuable when dealing with imbalanced datasets because it penalizes models that favor one metric over the other. A high F1-score indicates that the model has good performance in both identifying defaulters (recall) and not incorrectly flagging non-defaulters (precision). For imbalanced datasets, a *weighted F1-score* might be preferred, which calculates the F1-score for each class independently and then averages them, weighted by the number of true instances for each class.\n",
        "\n",
        "#### 5. AUC-ROC Curve (Area Under the Receiver Operating Characteristic Curve)\n",
        "The **AUC-ROC Curve** plots the True Positive Rate (Recall) against the False Positive Rate (FP / (FP + TN)) at various classification thresholds. The Area Under the Curve (AUC) provides a single scalar value that measures the model's ability to distinguish between positive and negative classes across all possible thresholds. An AUC of 1.0 indicates a perfect model, while an AUC of 0.5 suggests a model performing no better than random guessing.\n",
        "\n",
        "Its robustness to class imbalance makes it a vital metric for loan default prediction. It helps to understand the trade-off between identifying actual defaulters and incorrectly flagging non-defaulters, regardless of the chosen probability threshold. This allows the FinTech company to select an optimal threshold based on their specific risk tolerance and business objectives.\n",
        "\n",
        "#### 6. AUC-PR Curve (Area Under the Precision-Recall Curve)\n",
        "For highly imbalanced datasets, where the positive class (loan defaults) is rare and of primary interest, the **AUC-PR Curve** is often considered a more informative alternative or complement to the AUC-ROC curve. It plots Precision against Recall at various classification thresholds. The AUC-PR focuses specifically on the performance of the minority (positive) class, providing a clearer picture of how well the model identifies positive instances without being overly optimistic due to a large number of true negatives. A high AUC-PR indicates that the model is performing well in finding actual defaulters while maintaining a reasonable level of precision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ef78ad9"
      },
      "source": [
        "## Business Benefits\n",
        "\n",
        "### Subtask:\n",
        "Describe how a well-performing loan default prediction model would benefit the FinTech company. This should cover aspects like risk mitigation, optimized lending decisions, reduced financial losses, improved customer segmentation, and compliance benefits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "603986ca"
      },
      "source": [
        "## Business Benefits of a Loan Default Prediction Model for a FinTech Company\n",
        "\n",
        "A well-performing loan default prediction model is an invaluable asset for a FinTech company, offering significant strategic and operational advantages. By accurately assessing the likelihood of a borrower defaulting, the model empowers the company to make data-driven decisions that enhance profitability, mitigate risks, and improve customer relationships.\n",
        "\n",
        "Here's how such a model provides benefits:\n",
        "\n",
        "### 1. Risk Mitigation\n",
        "\n",
        "At its core, the primary benefit of a loan default prediction model is **risk mitigation**. The model accurately identifies high-risk applicants who are more likely to default on their loans. By flagging these applicants upfront, the company can either decline the loan application, offer the loan under more stringent terms (e.g., higher interest rates, shorter repayment periods), or require additional collateral. This proactive approach significantly reduces the probability of loan defaults and, consequently, the associated financial losses from non-performing loans. It shifts the company from a reactive stance (dealing with defaults after they occur) to a preventative one.\n",
        "\n",
        "### 2. Optimized Lending Decisions\n",
        "\n",
        "The model enables **optimized lending decisions** by providing a clear, quantitative assessment of each applicant's risk profile. This allows the FinTech company to:\n",
        "\n",
        "*   **Approve more creditworthy applicants:** By confidently identifying low-risk borrowers, the company can extend credit to a broader segment of the market that might otherwise be overlooked by traditional, often more conservative, lending criteria. This expands the customer base and increases revenue opportunities.\n",
        "*   **Decline high-risk ones:** The model provides objective grounds to decline applications from individuals with a high propensity to default, preventing potential losses.\n",
        "*   **Fine-tune loan terms:** For applicants with moderate risk, the model can inform the setting of appropriate interest rates, loan amounts, and repayment schedules that balance risk and return. This maximizes profit margins while minimizing default rates across the portfolio.\n",
        "\n",
        "### 3. Reduced Financial Losses\n",
        "\n",
        "Directly stemming from improved risk mitigation and optimized lending decisions, a robust prediction model leads to **reduced financial losses**. This includes:\n",
        "\n",
        "*   **Lower write-offs:** Fewer defaults mean fewer loans that need to be written off as unrecoverable.\n",
        "*   **Decreased collection costs:** The resources spent on chasing defaulted loans, engaging collection agencies, or pursuing legal action are significantly reduced.\n",
        "*   **Improved cash flow:** Predictable and lower default rates lead to more stable and improved cash flow for the company.\n",
        "*   **Better capital allocation:** Capital can be allocated more efficiently to profitable ventures rather than being tied up in problematic loans.\n",
        "\n",
        "### 4. Improved Customer Segmentation and Personalization\n",
        "\n",
        "The insights gained from the default prediction model can be leveraged for **improved customer segmentation**. By understanding the varying risk profiles and behavioral patterns associated with different segments of customers, the company can:\n",
        "\n",
        "*   **Tailor loan products:** Develop specific loan products designed for different risk tiers, making them more attractive and suitable for the respective segments.\n",
        "*   **Personalize interest rates:** Offer dynamic and personalized interest rates based on the predicted default risk, ensuring fair pricing for borrowers and appropriate risk-adjusted returns for the company.\n",
        "*   **Targeted marketing strategies:** Focus marketing efforts on creditworthy segments and develop retention strategies for valuable customers, while designing specific interventions or educational programs for potentially at-risk segments before they default.\n",
        "\n",
        "### 5. Compliance Benefits\n",
        "\n",
        "Operating in a highly regulated industry, FinTech companies benefit from the model by demonstrating **robust risk assessment practices to regulatory bodies**. A transparent and data-driven approach to credit risk modeling can:\n",
        "\n",
        "*   **Satisfy regulatory requirements:** Many financial regulations mandate sophisticated risk management frameworks. A well-documented and validated default prediction model helps meet these requirements.\n",
        "*   **Enhance reputation:** Demonstrating strong internal controls and a commitment to responsible lending practices can improve the company's standing with regulators, investors, and the public.\n",
        "*   **Facilitate audits:** The model provides auditable evidence for credit decisions, making compliance audits smoother and more efficient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "395e594f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the proposed data science pipeline and its anticipated impact.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f67b50"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Data Preprocessing Strategy**:\n",
        "    *   **Missing Values**: For numerical features, median imputation is preferred for skewed data; for categorical features, treating missing values as a separate 'Unknown' category is recommended.\n",
        "    *   **Categorical Feature Encoding**: One-Hot Encoding is suitable for low-cardinality features. For high-cardinality features, target encoding (with careful mitigation of target leakage) or CatBoost's built-in Ordered Target Encoding is recommended.\n",
        "    *   **Class Imbalance Handling**: Directly adjusting class weights within boosting algorithms (cost-sensitive learning) is preferred over resampling methods, coupled with the use of imbalance-aware evaluation metrics.\n",
        "*   **Boosting Algorithm Selection**:\n",
        "    *   **CatBoost** is identified as the most suitable algorithm due to its native and robust handling of categorical features (Ordered Target Encoding, which prevents target leakage), automatic treatment of missing values, and strong regularization.\n",
        "    *   **XGBoost** is a strong alternative, offering flexibility, scalability, and native missing value handling, but requires external preprocessing for categorical features.\n",
        "    *   **AdaBoost** is deemed less suitable due to its sensitivity to noise/outliers and lack of native handling for missing values or categorical features.\n",
        "*   **Hyperparameter Tuning Strategy**:\n",
        "    *   **Methods**: `RandomizedSearchCV` is recommended for large search spaces to efficiently find good hyperparameter combinations, while `GridSearchCV` is suitable for smaller, more exhaustive searches.\n",
        "    *   **Key Parameters**: Search spaces for `learning_rate`, `n_estimators`/`iterations`, `max_depth`/`depth`, regularization terms, and class weight parameters are outlined for both XGBoost and CatBoost.\n",
        "    *   **Cross-Validation**: K-Fold cross-validation is essential for robust performance estimation, with `TimeSeriesSplit` recommended for datasets with temporal dependencies.\n",
        "*   **Evaluation Metrics for Imbalanced Data**:\n",
        "    *   Standard accuracy is insufficient and misleading for loan default prediction due to class imbalance.\n",
        "    *   **Confusion Matrix**: Provides a fundamental understanding of True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN), which are critical for business decisions (e.g., FN represents approving a loan that defaults, leading to financial loss).\n",
        "    *   **Precision**: Important for minimizing False Positives (wrongly denying creditworthy applicants, leading to lost revenue).\n",
        "    *   **Recall**: Crucial for minimizing False Negatives (identifying as many actual defaulters as possible to mitigate financial losses).\n",
        "    *   **F1-Score**: Balances Precision and Recall, with a weighted F1-score preferred for imbalanced datasets.\n",
        "    *   **AUC-ROC**: Measures the model's discriminative ability across all thresholds, robust to class imbalance.\n",
        "    *   **AUC-PR**: Particularly valuable for highly imbalanced datasets, as it focuses on the performance of the minority (positive) class.\n",
        "*   **Business Benefits**: A robust loan default prediction model provides significant advantages, including enhanced risk mitigation, optimized lending decisions, reduced financial losses, improved customer segmentation and personalization, and compliance benefits by demonstrating strong risk assessment practices.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The pipeline prioritizes **CatBoost** for its native handling of categorical features and missing values, which will simplify preprocessing and reduce the risk of data leakage, crucial for FinTech data.\n",
        "*   The use of **cost-sensitive learning** (class weights) and **imbalance-aware evaluation metrics** like AUC-PR and weighted F1-score is fundamental to building a model that effectively identifies loan defaulters without being misled by the rare occurrence of default events.\n"
      ]
    }
  ]
}